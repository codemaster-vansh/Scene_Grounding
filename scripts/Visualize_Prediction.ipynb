{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2cad6a",
   "metadata": {},
   "source": [
    "# Visualising Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc5b149",
   "metadata": {},
   "source": [
    "### 1. Imports and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78449bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from utils import FinalModel, visualize_prediction\n",
    "from config_io import save_to_config, get_config_value\n",
    "from transformers import RobertaModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\",add_prefix_space=True)\n",
    "text_encoder = RobertaModel.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4190c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "choices = [\"final_weights.pth\",\"best_weights.pth\"]      #Choose\n",
    "i = 0\n",
    "model_save_weight_path = None\n",
    "ckpt_dir  = Path(get_config_value(\"CHECKPOINTS_PATH\", default=\"not-set\"))\n",
    "candidate = ckpt_dir / choices[i]\n",
    "\n",
    "if not candidate.is_file():                    # ← real existence test\n",
    "    print(f\"Please provide the link for {choices[i]}\")\n",
    "    model_save_weight_path = Path(\"/content/drive/MyDrive/refcoco_project/Weights_Checkpoint/best_weights.pth\")\n",
    "    save_to_config({\"CHECKPOINTS_PATH\": str(model_save_weight_path.parent)})\n",
    "else:\n",
    "    model_save_weight_path = candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2859df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinalModel()\n",
    "state_dict = torch.load(model_save_weight_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740fa4d",
   "metadata": {},
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"\"\n",
    "filename = \"\"\n",
    "\n",
    "\n",
    "base_dir      = Path(get_config_value(\"OUT_DIR\", default=\"not-set\"))\n",
    "img_path      = base_dir / filename\n",
    "\n",
    "if not img_path.is_file():\n",
    "    new_dir = Path(\n",
    "        input(\n",
    "            f\"File '{img_path}' not found.\\n\"\n",
    "            f\"Enter the directory that contains '{filename}': \"\n",
    "        ).strip()\n",
    "    ).expanduser().resolve()\n",
    "\n",
    "    # sanity-check the new directory\n",
    "    if not new_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"‘{new_dir}’ is not a directory.\")\n",
    "    \n",
    "    save_to_config({\"TEST_IMG_DIR\": str(new_dir)})\n",
    "    \n",
    "    img_path = new_dir / filename\n",
    "    if not img_path.is_file():\n",
    "        raise FileNotFoundError(f\"‘{img_path}’ still does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f56033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this cell to see the image before predicted boxes\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_np = np.array(img)\n",
    "plt.imshow(img_np)\n",
    "plt.axis('off')\n",
    "plt.title(\"The image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ac941",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(model,img_path,phrase,tokenizer,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
